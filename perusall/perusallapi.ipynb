{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dff6e23a",
   "metadata": {},
   "source": [
    "# Perusall API Process\n",
    "\n",
    "Using perusall API requires several steps and some pre-processing to get the pure text.\n",
    "\n",
    "### Step 1: Find Course ID\n",
    "\n",
    "curl https://app.perusall.com/api/v1/courses \\\n",
    "-H 'X-Institution: YOUR_INSTITUTION_ID' \\\n",
    "-H 'X-API-Token: YOUR_API_TOKEN'\n",
    "\n",
    "This returns a list of all courses with their IDs and names:\n",
    "\n",
    "  {\n",
    "    \"_id\": \"DR5mJTuukAD3pyLPn\",\n",
    "    \"name\": \"InkSpire\"\n",
    "  }\n",
    "\n",
    "  ### Step 2: Find Document ID\n",
    "To get document IDs, you need to use the Get the contents of a course library endpoint with a specific course ID:\n",
    "\n",
    "  curl https://app.perusall.com/api/v1/courses/YOUR_COURSE_ID/library \\\n",
    "-H 'X-Institution: YOUR_INSTITUTION_ID' \\\n",
    "-H 'X-API-Token: YOUR_API_TOKEN'\n",
    "\n",
    "This returns a list of all documents in the course library:\n",
    "\n",
    "  {\n",
    "    \"_id\": \"bYxScPDcw6gLyWedC\",\n",
    "    \"name\": \"reading1\",\n",
    "    \"format\": \"pdf\",\n",
    "    \"fromCatalog\": false\n",
    "  },\n",
    "  {\n",
    "    \"_id\": \"qhML9bQ3bnJgPQd6m\",\n",
    "    \"name\": \"The essence of constructivism\",\n",
    "    \"format\": \"pdf\",\n",
    "    \"fromCatalog\": false\n",
    "  }\n",
    "\n",
    "  ### Step 3: Get Document Content Metadata\n",
    "\n",
    "  curl https://app.perusall.com/api/v1/courses/YOUR_COURSE_ID/library/YOUR_DOCUMENT_ID \\\n",
    "-H 'X-Institution: YOUR_INSTITUTION_ID' \\\n",
    "-H 'X-API-Token: YOUR_API_TOKEN'\n",
    "\n",
    "This returns metadata including temporary URLs for each page:\n",
    "\n",
    "  \"_id\": \"qhML9bQ3bnJgPQd6m\",\n",
    "  \"name\": \"The essence of constructivism\",\n",
    "  \"format\": \"pdf\",\n",
    "  \"pages\": [\n",
    "    {\n",
    "      \"number\": 1,\n",
    "      \"width\": 1768,\n",
    "      \"height\": 2652,\n",
    "      \"textContentUrl\": \"https://d12klv9dmumy6j.cloudfront.net/text-content/qhML9bQ3bnJgPQd6m/QzgbyMwdYkvuuauhY.json?Expires=1757094820&Key-Pair-Id=APKAJUNGUCXEMBUZLHPQ&Signature=eddGsZA0ykAt-rGnkurK5S-3ladpbLeTQt8Z2iM3Li4jiy7yPM7bmruEcZt4S96MkG~EZreONd4phZ2-mcGqrjA-2eV9871TETiTnfSER~sRxTvUa7xa4g3oILJS-sdGpTXHgJ9TvMGIXc66EW1E5oQhFzSh1rJmYAEB2vJRsPY7xISgzmACwT1LJmMOrRKsSIjkoIzPeuWQ5fKpsR4cF62xQYBE9pK9IJsTB59gl9Rd4QdYH4mh03nu107Klt9zazu6b2ePwev8ZYQtkL5SNYk8qY2eOALxyryp2MK7Dt4dcfqvvG2jTklO5SmR4NR~TtQPmppOR7UUiZL8UaJEQA__\",\n",
    "      \"thumbnailUrl\": \"https://d12klv9dmumy6j.cloudfront.net/pages/QzgbyMwdYkvuuauhY-thumbnail.png?Expires=1757094820&Key-Pair-Id=APKAJUNGUCXEMBUZLHPQ&Signature=DH1cPIFW2VZAywEuQQaqbd6hJi6AoS9NGu1Ge3niwAA5bDzXZNOblK1VnIrMHttYIa-Sd3mo8u~5Rnk8RUQvBrHQU~3s7~CYV0qWj-ii2lQXenh42tyOjO~kEZyHeDum3zkc9g-Lf7NueTKFw1YM15laisnw0D2hOC-~SJKcrEuQwP1bZtA~47TKmOewGe-PqbHoRpGg1tnxOHNcxKVoyCgaYskWKLwUSiEcB2N9KWQH4IjXhAe1klWSDHDFmHFwcG4b3D7ISC2g0COQjbul4iz1A2eBFNPJRmx7VO5coCnGgwUG9ffrdrCx78xlLCpYi2dGnAuU9dz7fTgAa7uV0w__\",\n",
    "      \"expiresAt\": \"2025-09-05T17:53:40.077Z\"\n",
    "    },\n",
    "    {\n",
    "      \"number\": 2,\n",
    "      \"width\": 1768,\n",
    "      \"height\": 2652,\n",
    "      \"textContentUrl\": \"https://d12klv9dmumy6j.cloudfront.net/text-content/qhML9bQ3bnJgPQd6m/YYMn7zGZZEkkooLau.json?Expires=1757094823&Key-Pair-Id=APKAJUNGUCXEMBUZLHPQ&Signature=U3loCEzDQ12zFlTihaQo6NY7xzref61j2Z0kua2njaKwiwFLgiZH8Kh7yduBbOpDn35EIaQx~7Dz4fD8bqiMA2MGJe10bxYSZw5JEqJGM9rm44KufRGwKOTx3y~nal5oDNzzcrGf2mpe4uNmui64Jszg6q0tIwEOFRQmQbpjdg0BAuoqkliQrfhKcC92FMicdHikBgW4zLjhwaPFXkoibSd~BJyZjB6zIz0TgS9DlfXRi5QQwDQrxvQhxiH6uejU~k6sW~tqWSAVW8N4dY2PJOp8022AB6Dzr1HlYgUPYvlBKQcTQ5IB8HUmIQY1GrAn5Eur1uMM6OFPKiK56wzZGA__\",\n",
    "      \"thumbnailUrl\": \"https://d12klv9dmumy6j.cloudfront.net/pages/YYMn7zGZZEkkooLau-thumbnail.png?Expires=1757094823&Key-Pair-Id=APKAJUNGUCXEMBUZLHPQ&Signature=Smf17mvDklIoKMr~C9o430pZcmmrE-wAqij84g8VwnwBbOEGDacJe5Uf60hSwhb~Fy44azBMbF2lwH3DUheVnwgjGQx6HxrV6n4e98Jx0urWEdYisxcgfgnWtbsOisBieEZNhLobyDufz7bVZYnRXDrV7qMHW9ak02fLi9A2nMsaBbxu9hSYmLsO8NmYb4J22UvjW2MmlJaQhZIpIXBf8VbDzbwoRCZZzePJ1wOrCqbtLaC5QMkDpoD6d7sYGzTQeqmtjwvlpOdmqZbNceug8mF6wTC7x3U0nHnmObygRY6RVORZl6MpJ~s~neQoEgOXe9EQZvvM7-Qccm9bZfVMDw__\",\n",
    "      \"expiresAt\": \"2025-09-05T17:53:43.382Z\"\n",
    "    }\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0c6db6",
   "metadata": {},
   "source": [
    "  ### Step 4: Access the Text Content\n",
    "  For each page, use the textContentUrl to get the actual text content:\n",
    "\n",
    "  curl \"THE_TEXT_CONTENT_URL_FROM_STEP_1\"\n",
    "\n",
    "  This will return the JSON file for the page content. \n",
    "\n",
    "  For multi-page articles, we can use this code to retrieve all the output (saved as perusall_data.json):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3b9faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from typing import Dict, Any\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "import os\n",
    "import glob\n",
    "\n",
    "class DirectPerusallExtractor:\n",
    "    def __init__(self, delay_seconds: float = 1.0):\n",
    "        \"\"\"\n",
    "        Initialize the extractor\n",
    "        \n",
    "        Args:\n",
    "            delay_seconds: Delay between API requests to be respectful\n",
    "        \"\"\"\n",
    "        self.delay_seconds = delay_seconds\n",
    "        self.session = requests.Session()\n",
    "        # Add realistic headers\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'application/json, text/plain, */*',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'DNT': '1',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1'\n",
    "        })\n",
    "    \n",
    "    def load_json_file(self, file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Load and validate the Perusall JSON file\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read().strip()\n",
    "            \n",
    "            # Handle case where file might not start with { (incomplete JSON)\n",
    "            if not content.startswith('{'):\n",
    "                # Try to find the start of JSON\n",
    "                json_start = content.find('{')\n",
    "                if json_start != -1:\n",
    "                    content = content[json_start:]\n",
    "                else:\n",
    "                    raise ValueError(\"No valid JSON found in file\")\n",
    "            \n",
    "            # Handle case where file might not end with } (incomplete JSON)\n",
    "            if not content.endswith('}') and not content.endswith(']'):\n",
    "                # Try to find the last valid JSON ending\n",
    "                for ending in ['}', ']']:\n",
    "                    last_pos = content.rfind(ending)\n",
    "                    if last_pos != -1:\n",
    "                        content = content[:last_pos + 1]\n",
    "                        break\n",
    "            \n",
    "            data = json.loads(content)\n",
    "            \n",
    "            # Validate that we have the expected structure\n",
    "            if not isinstance(data, dict):\n",
    "                raise ValueError(\"JSON must be a dictionary/object\")\n",
    "            \n",
    "            if 'pages' not in data:\n",
    "                raise ValueError(\"JSON must contain a 'pages' array\")\n",
    "            \n",
    "            if not isinstance(data['pages'], list):\n",
    "                raise ValueError(\"'pages' must be an array\")\n",
    "            \n",
    "            if len(data['pages']) == 0:\n",
    "                raise ValueError(\"'pages' array is empty\")\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Invalid JSON format: {e}\")\n",
    "    \n",
    "    def is_url_expired(self, expires_at: str) -> bool:\n",
    "        \"\"\"Check if URL has expired\"\"\"\n",
    "        try:\n",
    "            if expires_at.endswith('Z'):\n",
    "                expire_time = datetime.fromisoformat(expires_at.replace('Z', '+00:00'))\n",
    "            else:\n",
    "                expire_time = datetime.fromisoformat(expires_at)\n",
    "            \n",
    "            if expire_time.tzinfo is None:\n",
    "                expire_time = expire_time.replace(tzinfo=timezone.utc)\n",
    "            \n",
    "            current_time = datetime.now(timezone.utc)\n",
    "            return current_time > expire_time\n",
    "            \n",
    "        except (ValueError, TypeError):\n",
    "            return False  # Assume not expired if we can't parse\n",
    "    \n",
    "    def extract_text_from_response(self, response_data: Dict[str, Any]) -> str:\n",
    "        \"\"\"Extract text from various possible response formats\"\"\"\n",
    "        text_content = \"\"\n",
    "        \n",
    "        # Method 1: Direct text field\n",
    "        if 'text' in response_data:\n",
    "            if isinstance(response_data['text'], str):\n",
    "                text_content = response_data['text']\n",
    "            elif isinstance(response_data['text'], list):\n",
    "                text_parts = []\n",
    "                for item in response_data['text']:\n",
    "                    if isinstance(item, str):\n",
    "                        text_parts.append(item)\n",
    "                    elif isinstance(item, dict):\n",
    "                        # Look for text in common fields\n",
    "                        for field in ['content', 'text', 'value', 'data']:\n",
    "                            if field in item:\n",
    "                                text_parts.append(str(item[field]))\n",
    "                                break\n",
    "                    else:\n",
    "                        text_parts.append(str(item))\n",
    "                text_content = ' '.join(text_parts)\n",
    "        \n",
    "        # Method 2: Content field\n",
    "        elif 'content' in response_data:\n",
    "            text_content = str(response_data['content'])\n",
    "        \n",
    "        # Method 3: Data field with text items\n",
    "        elif 'data' in response_data:\n",
    "            data = response_data['data']\n",
    "            if isinstance(data, list):\n",
    "                text_parts = []\n",
    "                for item in data:\n",
    "                    if isinstance(item, dict):\n",
    "                        # Look for text in the item\n",
    "                        for field in ['text', 'content', 'value']:\n",
    "                            if field in item:\n",
    "                                text_parts.append(str(item[field]))\n",
    "                                break\n",
    "                    else:\n",
    "                        text_parts.append(str(item))\n",
    "                text_content = ' '.join(text_parts)\n",
    "            else:\n",
    "                text_content = str(data)\n",
    "        \n",
    "        # Method 4: Look for any field that might contain text\n",
    "        else:\n",
    "            possible_fields = ['textContent', 'body', 'html', 'plain', 'raw', 'items']\n",
    "            for field in possible_fields:\n",
    "                if field in response_data:\n",
    "                    if isinstance(response_data[field], list):\n",
    "                        text_content = ' '.join(str(item) for item in response_data[field])\n",
    "                    else:\n",
    "                        text_content = str(response_data[field])\n",
    "                    break\n",
    "        \n",
    "        return text_content.strip()\n",
    "    \n",
    "    def fetch_page_text(self, url: str, page_number: int) -> str:\n",
    "        \"\"\"Fetch text content for a single page\"\"\"\n",
    "        try:\n",
    "            print(f\"    Requesting: {url[:100]}...\")\n",
    "            \n",
    "            response = self.session.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Try to parse as JSON\n",
    "            try:\n",
    "                data = response.json()\n",
    "                text = self.extract_text_from_response(data)\n",
    "                \n",
    "                if text:\n",
    "                    print(f\"    ‚úì Extracted {len(text)} characters\")\n",
    "                    return text\n",
    "                else:\n",
    "                    print(f\"    ‚ö† No text found in response\")\n",
    "                    print(f\"    Response keys: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}\")\n",
    "                    return \"\"\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                # If not JSON, treat as plain text\n",
    "                text = response.text.strip()\n",
    "                if text:\n",
    "                    print(f\"    ‚úì Got plain text: {len(text)} characters\")\n",
    "                    return text\n",
    "                else:\n",
    "                    print(f\"    ‚ö† Empty response\")\n",
    "                    return \"\"\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"    ‚úó Timeout after 30 seconds\")\n",
    "            return \"\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"    ‚úó Request failed: {e}\")\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚úó Unexpected error: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def extract_document(self, json_file_path: str, output_file: str = None) -> str:\n",
    "        \"\"\"Extract text from the entire document\"\"\"\n",
    "        \n",
    "        print(f\"Loading Perusall JSON: {json_file_path}\")\n",
    "        data = self.load_json_file(json_file_path)\n",
    "        \n",
    "        doc_name = data.get('name', 'Unknown Document')\n",
    "        doc_id = data.get('_id', 'unknown')\n",
    "        pages = data['pages']\n",
    "        \n",
    "        print(f\"Document: {doc_name}\")\n",
    "        print(f\"ID: {doc_id}\")\n",
    "        print(f\"Pages: {len(pages)}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        extracted_pages = []\n",
    "        successful = 0\n",
    "        failed = 0\n",
    "        expired = 0\n",
    "        \n",
    "        for i, page in enumerate(pages, 1):\n",
    "            page_num = page.get('number', i)\n",
    "            print(f\"\\nPage {page_num} ({i}/{len(pages)}):\")\n",
    "            \n",
    "            if 'textContentUrl' not in page:\n",
    "                print(f\"    ‚úó No textContentUrl found\")\n",
    "                failed += 1\n",
    "                continue\n",
    "            \n",
    "            # Check expiration\n",
    "            if 'expiresAt' in page and self.is_url_expired(page['expiresAt']):\n",
    "                print(f\"    ‚úó URL expired at {page['expiresAt']}\")\n",
    "                expired += 1\n",
    "                continue\n",
    "            \n",
    "            # Fetch the text\n",
    "            text = self.fetch_page_text(page['textContentUrl'], page_num)\n",
    "            \n",
    "            if text:\n",
    "                extracted_pages.append(f\"=== PAGE {page_num} ===\\n\\n{text}\")\n",
    "                successful += 1\n",
    "            else:\n",
    "                failed += 1\n",
    "            \n",
    "            # Be respectful with delays\n",
    "            if i < len(pages):\n",
    "                time.sleep(self.delay_seconds)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"EXTRACTION SUMMARY:\")\n",
    "        print(f\"  ‚úì Successful: {successful}\")\n",
    "        print(f\"  ‚úó Failed: {failed}\")\n",
    "        print(f\"  ‚è∞ Expired: {expired}\")\n",
    "        print(f\"  üìÑ Total pages: {len(pages)}\")\n",
    "        \n",
    "        if not extracted_pages:\n",
    "            raise ValueError(\"No pages could be extracted. Check if URLs have expired or are accessible.\")\n",
    "        \n",
    "        # Combine all pages\n",
    "        full_text = f\"DOCUMENT: {doc_name}\\nSOURCE: Perusall Export\\nEXTRACTED: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\" + \"\\n\\n\".join(extracted_pages)\n",
    "        \n",
    "        # Save if requested\n",
    "        if output_file:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(full_text)\n",
    "            print(f\"\\nüíæ Saved to: {output_file}\")\n",
    "        \n",
    "        return full_text\n",
    "\n",
    "def find_json_files():\n",
    "    \"\"\"Find all JSON files in the current directory\"\"\"\n",
    "    json_files = glob.glob(\"*.json\")\n",
    "    return json_files\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main extraction function\"\"\"\n",
    "    print(\"üîç Perusall Text Extractor\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Look for JSON files\n",
    "    json_files = find_json_files()\n",
    "    \n",
    "    if not json_files:\n",
    "        print(\"‚ùå No JSON files found in current directory.\")\n",
    "        print(\"\\nüí° Please:\")\n",
    "        print(\"   1. Make sure your Perusall JSON file is in this folder\")\n",
    "        print(\"   2. Make sure it has a .json extension\")\n",
    "        print(\"   3. Common names: perusall_data.json, document.json, etc.\")\n",
    "        return\n",
    "    \n",
    "    # If only one JSON file, use it\n",
    "    if len(json_files) == 1:\n",
    "        json_file = json_files[0]\n",
    "        print(f\"üìÑ Found JSON file: {json_file}\")\n",
    "    else:\n",
    "        # Let user choose\n",
    "        print(f\"üìÑ Found {len(json_files)} JSON files:\")\n",
    "        for i, file in enumerate(json_files, 1):\n",
    "            print(f\"   {i}. {file}\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                choice = input(f\"\\nSelect file (1-{len(json_files)}): \").strip()\n",
    "                idx = int(choice) - 1\n",
    "                if 0 <= idx < len(json_files):\n",
    "                    json_file = json_files[idx]\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Please enter a number between 1 and {len(json_files)}\")\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid number\")\n",
    "    \n",
    "    # Generate output filename\n",
    "    base_name = os.path.splitext(json_file)[0]\n",
    "    output_file = f\"{base_name}_extracted.txt\"\n",
    "    \n",
    "    print(f\"\\nüöÄ Starting extraction...\")\n",
    "    print(f\"üì• Input:  {json_file}\")\n",
    "    print(f\"üì§ Output: {output_file}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Extract the text\n",
    "        extractor = DirectPerusallExtractor(delay_seconds=0.8)\n",
    "        text = extractor.extract_document(json_file, output_file)\n",
    "        \n",
    "        print(f\"\\nüéâ EXTRACTION COMPLETE!\")\n",
    "        print(f\"üìä Characters: {len(text):,}\")\n",
    "        print(f\"üìä Words: {len(text.split()):,}\")\n",
    "        print(f\"üìÅ Saved to: {output_file}\")\n",
    "        \n",
    "        # Show a preview\n",
    "        print(f\"\\nüìñ PREVIEW:\")\n",
    "        print(\"=\" * 50)\n",
    "        lines = text.split('\\n')[:15]  # First 15 lines\n",
    "        for line in lines:\n",
    "            print(line[:100] + (\"...\" if len(line) > 100 else \"\"))\n",
    "        \n",
    "        if len(text.split('\\n')) > 15:\n",
    "            print(f\"... (and {len(text.split('\\n')) - 15} more lines)\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Done! You can now open the .txt file to read the content.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {e}\")\n",
    "        \n",
    "        # Provide helpful troubleshooting\n",
    "        print(f\"\\nüîß Troubleshooting:\")\n",
    "        print(f\"   ‚Ä¢ Make sure '{json_file}' is a valid Perusall JSON export\")\n",
    "        print(f\"   ‚Ä¢ Check your internet connection\")\n",
    "        print(f\"   ‚Ä¢ The URLs in the JSON file might have expired\")\n",
    "        print(f\"   ‚Ä¢ Try re-exporting from Perusall if URLs are old\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6308972f",
   "metadata": {},
   "source": [
    "### Step 5: Get the Pure Text\n",
    "Step 4 retrieves all the json contents of all pages together (saved as perusall_data_extracted.txt). But we still need to clean them to get pure text. So we use this clean text tool to get the pure text.\n",
    "\n",
    "Run python cleantext.py perusall_data_extracted.txt and we will get the final processed pure text in perusall_data_extracted_cleaned.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbe0dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "def clean_perusall_file(input_path, output_path=None):\n",
    "    \"\"\"Clean Perusall export and extract readable text.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Read the input file\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Extract text using regex\n",
    "        pattern = r\"'str':\\s*'([^']*)'\"\n",
    "        matches = re.findall(pattern, content)\n",
    "        \n",
    "        # Filter out empty/meaningless text\n",
    "        clean_text_pieces = []\n",
    "        for match in matches:\n",
    "            text = match.strip()\n",
    "            if text and len(text) > 0 and text not in [' ', '', '\\n', '\\t']:\n",
    "                clean_text_pieces.append(text)\n",
    "        \n",
    "        # Join text and clean spacing\n",
    "        result = ' '.join(clean_text_pieces)\n",
    "        result = re.sub(r'\\s+', ' ', result).strip()\n",
    "        \n",
    "        # Add paragraph breaks for readability\n",
    "        result = re.sub(r'\\. ([A-Z])', r'.\\n\\n\\1', result)\n",
    "        \n",
    "        # Save or return result\n",
    "        if output_path:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(result)\n",
    "            print(f\"‚úÖ Text cleaned and saved to: {output_path}\")\n",
    "            print(f\"üìä Original file: {len(content):,} characters\")\n",
    "            print(f\"üìä Cleaned text: {len(result):,} characters\")\n",
    "            print(f\"üìä Extracted {len(clean_text_pieces)} text segments\")\n",
    "        else:\n",
    "            return result\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: File '{input_path}' not found\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python clean_text.py input_file.txt [output_file.txt]\")\n",
    "        print(\"Example: python clean_text.py paste.txt cleaned_output.txt\")\n",
    "        return\n",
    "    \n",
    "    input_file = sys.argv[1]\n",
    "    output_file = sys.argv[2] if len(sys.argv) > 2 else input_file.replace('.txt', '_cleaned.txt')\n",
    "    \n",
    "    clean_perusall_file(input_file, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
